{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRkowTJjSp_L",
        "outputId": "784e2756-0e70-4ead-d15f-c0161b333aa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bag-of-Words (CountVectorizer) ===\n",
            "Vocabulary: ['and' 'brown' 'dog' 'fox' 'is' 'jump' 'jumps' 'lazy' 'never' 'over'\n",
            " 'quick' 'quickly' 'the']\n",
            "BoW Matrix:\n",
            " [[0 1 1 1 0 0 1 1 0 1 1 0 2]\n",
            " [0 0 1 0 0 1 0 1 1 1 0 1 1]\n",
            " [1 0 1 1 2 0 0 1 0 0 1 0 2]]\n",
            "\n",
            "=== Bag-of-n-grams (bi-grams) using CountVectorizer ===\n",
            "Bi-gram Vocabulary: ['and the' 'brown fox' 'dog is' 'dog quickly' 'fox is' 'fox jumps'\n",
            " 'is lazy' 'is quick' 'jump over' 'jumps over' 'lazy dog' 'never jump'\n",
            " 'over the' 'quick and' 'quick brown' 'the dog' 'the fox' 'the lazy'\n",
            " 'the quick']\n",
            "Bi-gram Matrix:\n",
            " [[0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 1]\n",
            " [0 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0]\n",
            " [1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0]]\n",
            "\n",
            "=== Bag-of-Words (TF-IDF Vectorizer) ===\n",
            "TF-IDF Vocabulary: ['and' 'brown' 'dog' 'fox' 'is' 'jump' 'jumps' 'lazy' 'never' 'over'\n",
            " 'quick' 'quickly' 'the']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.41422296 0.24464675 0.31502724 0.         0.\n",
            "  0.41422296 0.24464675 0.         0.31502724 0.31502724 0.\n",
            "  0.4892935 ]\n",
            " [0.         0.         0.27463443 0.         0.         0.46499651\n",
            "  0.         0.27463443 0.46499651 0.35364183 0.         0.46499651\n",
            "  0.27463443]\n",
            " [0.34816031 0.         0.20562909 0.26478489 0.69632062 0.\n",
            "  0.         0.20562909 0.         0.         0.26478489 0.\n",
            "  0.41125817]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Never jump over the lazy dog quickly\",\n",
        "    \"The fox is quick and the dog is lazy\"\n",
        "]\n",
        "\n",
        "# a) Bag-of-Words using CountVectorizer\n",
        "print(\"=== Bag-of-Words (CountVectorizer) ===\")\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow = count_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"BoW Matrix:\\n\", bow.toarray())\n",
        "print()\n",
        "\n",
        "# b) Bag-of-n-grams (e.g., bi-grams) using CountVectorizer\n",
        "print(\"=== Bag-of-n-grams (bi-grams) using CountVectorizer ===\")\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))  # Bi-grams\n",
        "bigrams = ngram_vectorizer.fit_transform(corpus)\n",
        "print(\"Bi-gram Vocabulary:\", ngram_vectorizer.get_feature_names_out())\n",
        "print(\"Bi-gram Matrix:\\n\", bigrams.toarray())\n",
        "print()\n",
        "\n",
        "# c) Bag-of-Words using TfidfVectorizer\n",
        "print(\"=== Bag-of-Words (TF-IDF Vectorizer) ===\")\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(\"TF-IDF Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf.toarray())"
      ]
    }
  ]
}